{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJs5aQb3hWGNGxHMQ2DhhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlasrikanth/Real-Time-Multimodal-AI-for-Autonomous-Vehicle-Perception-and-Decision-Making/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrXny0AEQPZs",
        "outputId": "e666c48c-cf1d-40ae-a16a-13e134d1286e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['# Mount Google Drive\\n',\n",
              "    'from google.colab import drive\\n',\n",
              "    \"drive.mount('/content/drive')\\n\",\n",
              "    '\\n',\n",
              "    '# Install dependencies\\n',\n",
              "    '!pip install torch torchvision opencv-python-headless numpy\\n',\n",
              "    '!pip install ultralytics  # YOLOv8\\n',\n",
              "    '!pip install torch-geometric  # PointNet++\\n',\n",
              "    '!pip install stable-baselines3 gymnasium  # RL\\n',\n",
              "    '\\n',\n",
              "    '# Verify installations\\n',\n",
              "    'import torch\\n',\n",
              "    'print(torch.__version__, torch.cuda.is_available())\\n',\n",
              "    'from ultralytics import YOLO\\n',\n",
              "    'import cv2\\n',\n",
              "    'print(cv2.__version__)\\n',\n",
              "    '\\n',\n",
              "    '# Create directories for datasets and models\\n',\n",
              "    '!mkdir -p /content/drive/MyDrive/autonomous_ai/datasets\\n',\n",
              "    '!mkdir -p /content/drive/MyDrive/autonomous_ai/models\\n'],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# Mount Google Drive\\n\",\n",
        "        \"from google.colab import drive\\n\",\n",
        "        \"drive.mount('/content/drive')\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Install dependencies\\n\",\n",
        "        \"!pip install torch torchvision opencv-python-headless numpy\\n\",\n",
        "        \"!pip install ultralytics  # YOLOv8\\n\",\n",
        "        \"!pip install torch-geometric  # PointNet++\\n\",\n",
        "        \"!pip install stable-baselines3 gymnasium  # RL\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Verify installations\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"print(torch.__version__, torch.cuda.is_available())\\n\",\n",
        "        \"from ultralytics import YOLO\\n\",\n",
        "        \"import cv2\\n\",\n",
        "        \"print(cv2.__version__)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Create directories for datasets and models\\n\",\n",
        "        \"!mkdir -p /content/drive/MyDrive/autonomous_ai/datasets\\n\",\n",
        "        \"!mkdir -p /content/drive/MyDrive/autonomous_ai/models\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import os\\n\",\n",
        "        \"import cv2\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"from torch.utils.data import Dataset, DataLoader\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Download nuScenes mini dataset\\n\",\n",
        "        \"!wget -P /content/drive/MyDrive/autonomous_ai/datasets https://www.nuscenes.org/data/nuscenes-mini-v1.0.tar.gz\\n\",\n",
        "        \"!tar -xzf /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0.tar.gz -C /content/drive/MyDrive/autonomous_ai/datasets\\n\",\n",
        "        \"\\n\",\n",
        "        \"class NuScenesDataset(Dataset):\\n\",\n",
        "        \"    def __init__(self, data_dir, split='mini_train'):\\n\",\n",
        "        \"        from nuscenes.nuscenes import NuScenes\\n\",\n",
        "        \"        self.nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\\n\",\n",
        "        \"        self.samples = [s for s in self.nusc.sample if s['scene_token'] in self.nusc.get('scene', split)]\\n\",\n",
        "        \"        self.camera = 'CAM_FRONT'\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def __len__(self):\\n\",\n",
        "        \"        return len(self.samples)\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def __getitem__(self, idx):\\n\",\n",
        "        \"        sample = self.nusc.sample[idx]\\n\",\n",
        "        \"        # Load camera image\\n\",\n",
        "        \"        cam_data = self.nusc.get('sample_data', sample['data'][self.camera])\\n\",\n",
        "        \"        img_path = os.path.join(self.nusc.dataroot, cam_data['filename'])\\n\",\n",
        "        \"        img = cv2.imread(img_path)\\n\",\n",
        "        \"        img = cv2.resize(img, (640, 640))\\n\",\n",
        "        \"        img = torch.tensor(img.transpose(2, 0, 1), dtype=torch.float32) / 255.0\\n\",\n",
        "        \"        \\n\",\n",
        "        \"        # Load LiDAR point cloud (limit to 1000 points to fit RAM)\\n\",\n",
        "        \"        lidar_data = self.nusc.get('sample_data', sample['data']['LIDAR_TOP'])\\n\",\n",
        "        \"        lidar_path = os.path.join(self.nusc.dataroot, lidar_data['filename'])\\n\",\n",
        "        \"        points = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)[:1000, :3]\\n\",\n",
        "        \"        points = torch.tensor(points, dtype=torch.float32)\\n\",\n",
        "        \"        \\n\",\n",
        "        \"        # Load annotations\\n\",\n",
        "        \"        boxes = []\\n\",\n",
        "        \"        for ann_token in sample['anns']:\\n\",\n",
        "        \"            ann = self.nusc.get('sample_annotation', ann_token)\\n\",\n",
        "        \"            if ann['category_name'] in ['vehicle.car', 'human.pedestrian']:\\n\",\n",
        "        \"                box = ann['bbox']\\n\",\n",
        "        \"                boxes.append(box)\\n\",\n",
        "        \"        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\\n\",\n",
        "        \"        \\n\",\n",
        "        \"        return img, points, boxes\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Initialize dataset\\n\",\n",
        "        \"dataset = NuScenesDataset(data_dir='/content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0')\\n\",\n",
        "        \"dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # Smaller batch size for Colab\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04kQBe0lRS8t",
        "outputId": "27942e84-5a48-4d9b-ff0e-caca3565f38b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import os\\n',\n",
              "    'import cv2\\n',\n",
              "    'import numpy as np\\n',\n",
              "    'import torch\\n',\n",
              "    'from torch.utils.data import Dataset, DataLoader\\n',\n",
              "    '\\n',\n",
              "    '# Download nuScenes mini dataset\\n',\n",
              "    '!wget -P /content/drive/MyDrive/autonomous_ai/datasets https://www.nuscenes.org/data/nuscenes-mini-v1.0.tar.gz\\n',\n",
              "    '!tar -xzf /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0.tar.gz -C /content/drive/MyDrive/autonomous_ai/datasets\\n',\n",
              "    '\\n',\n",
              "    'class NuScenesDataset(Dataset):\\n',\n",
              "    \"    def __init__(self, data_dir, split='mini_train'):\\n\",\n",
              "    '        from nuscenes.nuscenes import NuScenes\\n',\n",
              "    \"        self.nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\\n\",\n",
              "    \"        self.samples = [s for s in self.nusc.sample if s['scene_token'] in self.nusc.get('scene', split)]\\n\",\n",
              "    \"        self.camera = 'CAM_FRONT'\\n\",\n",
              "    '    \\n',\n",
              "    '    def __len__(self):\\n',\n",
              "    '        return len(self.samples)\\n',\n",
              "    '    \\n',\n",
              "    '    def __getitem__(self, idx):\\n',\n",
              "    '        sample = self.nusc.sample[idx]\\n',\n",
              "    '        # Load camera image\\n',\n",
              "    \"        cam_data = self.nusc.get('sample_data', sample['data'][self.camera])\\n\",\n",
              "    \"        img_path = os.path.join(self.nusc.dataroot, cam_data['filename'])\\n\",\n",
              "    '        img = cv2.imread(img_path)\\n',\n",
              "    '        img = cv2.resize(img, (640, 640))\\n',\n",
              "    '        img = torch.tensor(img.transpose(2, 0, 1), dtype=torch.float32) / 255.0\\n',\n",
              "    '        \\n',\n",
              "    '        # Load LiDAR point cloud (limit to 1000 points to fit RAM)\\n',\n",
              "    \"        lidar_data = self.nusc.get('sample_data', sample['data']['LIDAR_TOP'])\\n\",\n",
              "    \"        lidar_path = os.path.join(self.nusc.dataroot, lidar_data['filename'])\\n\",\n",
              "    '        points = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)[:1000, :3]\\n',\n",
              "    '        points = torch.tensor(points, dtype=torch.float32)\\n',\n",
              "    '        \\n',\n",
              "    '        # Load annotations\\n',\n",
              "    '        boxes = []\\n',\n",
              "    \"        for ann_token in sample['anns']:\\n\",\n",
              "    \"            ann = self.nusc.get('sample_annotation', ann_token)\\n\",\n",
              "    \"            if ann['category_name'] in ['vehicle.car', 'human.pedestrian']:\\n\",\n",
              "    \"                box = ann['bbox']\\n\",\n",
              "    '                boxes.append(box)\\n',\n",
              "    '        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\\n',\n",
              "    '        \\n',\n",
              "    '        return img, points, boxes\\n',\n",
              "    '\\n',\n",
              "    '# Initialize dataset\\n',\n",
              "    \"dataset = NuScenesDataset(data_dir='/content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0')\\n\",\n",
              "    'dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # Smaller batch size for Colab\\n'],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"from ultralytics import YOLO\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Load pre-trained YOLOv8 nano model\\n\",\n",
        "        \"model = YOLO('yolov8n.pt')\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Custom training configuration\\n\",\n",
        "        \"data_config = \\\"\\\"\\\"\\n\",\n",
        "        \"train: /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0/train\\n\",\n",
        "        \"val: /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0/val\\n\",\n",
        "        \"nc: 2\\n\",\n",
        "        \"names: ['car', 'pedestrian']\\n\",\n",
        "        \"\\\"\\\"\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Save config\\n\",\n",
        "        \"with open('/content/nuscenes.yaml', 'w') as f:\\n\",\n",
        "        \"    f.write(data_config)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Train model\\n\",\n",
        "        \"model.train(data='/content/nuscenes.yaml', epochs=20, imgsz=640, batch=4, device=0)  # Reduced epochs and batch\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Save model to Drive\\n\",\n",
        "        \"model.save('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XwphYOeRmV6",
        "outputId": "c4aa687a-767a-4308-b8c6-c4f39c813de0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['from ultralytics import YOLO\\n',\n",
              "    'import torch\\n',\n",
              "    '\\n',\n",
              "    '# Load pre-trained YOLOv8 nano model\\n',\n",
              "    \"model = YOLO('yolov8n.pt')\\n\",\n",
              "    '\\n',\n",
              "    '# Custom training configuration\\n',\n",
              "    'data_config = \"\"\"\\n',\n",
              "    'train: /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0/train\\n',\n",
              "    'val: /content/drive/MyDrive/autonomous_ai/datasets/nuscenes-mini-v1.0/val\\n',\n",
              "    'nc: 2\\n',\n",
              "    \"names: ['car', 'pedestrian']\\n\",\n",
              "    '\"\"\"\\n',\n",
              "    '\\n',\n",
              "    '# Save config\\n',\n",
              "    \"with open('/content/nuscenes.yaml', 'w') as f:\\n\",\n",
              "    '    f.write(data_config)\\n',\n",
              "    '\\n',\n",
              "    '# Train model\\n',\n",
              "    \"model.train(data='/content/nuscenes.yaml', epochs=20, imgsz=640, batch=4, device=0)  # Reduced epochs and batch\\n\",\n",
              "    '\\n',\n",
              "    '# Save model to Drive\\n',\n",
              "    \"model.save('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\"],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import torch\\n\",\n",
        "        \"import torch.nn as nn\\n\",\n",
        "        \"from torch_geometric.nn import PointNetConv, global_max_pool\\n\",\n",
        "        \"from torch_geometric.data import Data, DataLoader as GeoDataLoader\\n\",\n",
        "        \"\\n\",\n",
        "        \"class PointNetSeg(nn.Module):\\n\",\n",
        "        \"    def __init__(self, num_classes=2):\\n\",\n",
        "        \"        super(PointNetSeg, self).__init__()\\n\",\n",
        "        \"        self.conv1 = PointNetConv(local_nn=nn.Sequential(\\n\",\n",
        "        \"            nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, 32)))  # Reduced size\\n\",\n",
        "        \"        self.conv2 = PointNetConv(local_nn=nn.Sequential(\\n\",\n",
        "        \"            nn.Linear(35, 64), nn.ReLU(), nn.Linear(64, 64)))\\n\",\n",
        "        \"        self.fc = nn.Linear(64, num_classes)\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def forward(self, data):\\n\",\n",
        "        \"        x, pos, batch = data.x, data.pos, data.batch\\n\",\n",
        "        \"        x = self.conv1(x, pos, batch)\\n\",\n",
        "        \"        x = self.conv2(x, pos, batch)\\n\",\n",
        "        \"        x = global_max_pool(x, batch)\\n\",\n",
        "        \"        x = self.fc(x)\\n\",\n",
        "        \"        return x\\n\",\n",
        "        \"\\n\",\n",
        "        \"def create_pointnet_data(points, labels):\\n\",\n",
        "        \"    return Data(pos=torch.tensor(points, dtype=torch.float32), y=torch.tensor(labels, dtype=torch.long))\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Training loop\\n\",\n",
        "        \"model = PointNetSeg(num_classes=2).cuda()\\n\",\n",
        "        \"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\",\n",
        "        \"criterion = nn.CrossEntropyLoss()\\n\",\n",
        "        \"\\n\",\n",
        "        \"for epoch in range(20):  # Reduced epochs\\n\",\n",
        "        \"    model.train()\\n\",\n",
        "        \"    for batch in dataloader:\\n\",\n",
        "        \"        points, labels = batch[1].cuda(), batch[2].cuda()\\n\",\n",
        "        \"        data_list = [create_pointnet_data(p, l[:1]) for p, l in zip(points, labels)]  # Simplified labels\\n\",\n",
        "        \"        batch_data = GeoDataLoader(data_list, batch_size=4)\\n\",\n",
        "        \"        optimizer.zero_grad()\\n\",\n",
        "        \"        out = model(batch_data)\\n\",\n",
        "        \"        loss = criterion(out, batch_data.y)\\n\",\n",
        "        \"        loss.backward()\\n\",\n",
        "        \"        optimizer.step()\\n\",\n",
        "        \"    print(f'Epoch {epoch}, Loss: {loss.item()}')\\n\",\n",
        "        \"\\n\",\n",
        "        \"torch.save(model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt')\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng-9HzplRu6W",
        "outputId": "76d5a6fa-f4d3-4ee3-9910-7ccd8cc9f4bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import torch\\n',\n",
              "    'import torch.nn as nn\\n',\n",
              "    'from torch_geometric.nn import PointNetConv, global_max_pool\\n',\n",
              "    'from torch_geometric.data import Data, DataLoader as GeoDataLoader\\n',\n",
              "    '\\n',\n",
              "    'class PointNetSeg(nn.Module):\\n',\n",
              "    '    def __init__(self, num_classes=2):\\n',\n",
              "    '        super(PointNetSeg, self).__init__()\\n',\n",
              "    '        self.conv1 = PointNetConv(local_nn=nn.Sequential(\\n',\n",
              "    '            nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, 32)))  # Reduced size\\n',\n",
              "    '        self.conv2 = PointNetConv(local_nn=nn.Sequential(\\n',\n",
              "    '            nn.Linear(35, 64), nn.ReLU(), nn.Linear(64, 64)))\\n',\n",
              "    '        self.fc = nn.Linear(64, num_classes)\\n',\n",
              "    '    \\n',\n",
              "    '    def forward(self, data):\\n',\n",
              "    '        x, pos, batch = data.x, data.pos, data.batch\\n',\n",
              "    '        x = self.conv1(x, pos, batch)\\n',\n",
              "    '        x = self.conv2(x, pos, batch)\\n',\n",
              "    '        x = global_max_pool(x, batch)\\n',\n",
              "    '        x = self.fc(x)\\n',\n",
              "    '        return x\\n',\n",
              "    '\\n',\n",
              "    'def create_pointnet_data(points, labels):\\n',\n",
              "    '    return Data(pos=torch.tensor(points, dtype=torch.float32), y=torch.tensor(labels, dtype=torch.long))\\n',\n",
              "    '\\n',\n",
              "    '# Training loop\\n',\n",
              "    'model = PointNetSeg(num_classes=2).cuda()\\n',\n",
              "    'optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n',\n",
              "    'criterion = nn.CrossEntropyLoss()\\n',\n",
              "    '\\n',\n",
              "    'for epoch in range(20):  # Reduced epochs\\n',\n",
              "    '    model.train()\\n',\n",
              "    '    for batch in dataloader:\\n',\n",
              "    '        points, labels = batch[1].cuda(), batch[2].cuda()\\n',\n",
              "    '        data_list = [create_pointnet_data(p, l[:1]) for p, l in zip(points, labels)]  # Simplified labels\\n',\n",
              "    '        batch_data = GeoDataLoader(data_list, batch_size=4)\\n',\n",
              "    '        optimizer.zero_grad()\\n',\n",
              "    '        out = model(batch_data)\\n',\n",
              "    '        loss = criterion(out, batch_data.y)\\n',\n",
              "    '        loss.backward()\\n',\n",
              "    '        optimizer.step()\\n',\n",
              "    \"    print(f'Epoch {epoch}, Loss: {loss.item()}')\\n\",\n",
              "    '\\n',\n",
              "    \"torch.save(model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt')\\n\"],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import torch\\n\",\n",
        "        \"import torch.nn as nn\\n\",\n",
        "        \"from ultralytics import YOLO\\n\",\n",
        "        \"\\n\",\n",
        "        \"class FusionTransformer(nn.Module):\\n\",\n",
        "        \"    def __init__(self, input_dim=68, hidden_dim=128, num_heads=4):  # Reduced size\\n\",\n",
        "        \"        super(FusionTransformer, self).__init__()\\n\",\n",
        "        \"        self.encoder = nn.TransformerEncoder(\\n\",\n",
        "        \"            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\\n\",\n",
        "        \"            num_layers=1  # Single layer for Colab\\n\",\n",
        "        \"        )\\n\",\n",
        "        \"        self.fc = nn.Linear(input_dim, 64)\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def forward(self, yolo_features, lidar_features):\\n\",\n",
        "        \"        combined = torch.cat((yolo_features, lidar_features), dim=-1)\\n\",\n",
        "        \"        fused = self.encoder(combined)\\n\",\n",
        "        \"        return self.fc(fused)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Load models\\n\",\n",
        "        \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\",\n",
        "        \"pointnet = PointNetSeg().cuda()\\n\",\n",
        "        \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt'))\\n\",\n",
        "        \"fusion_model = FusionTransformer().cuda()\\n\",\n",
        "        \"\\n\",\n",
        "        \"optimizer = torch.optim.Adam(fusion_model.parameters(), lr=0.001)\\n\",\n",
        "        \"criterion = nn.MSELoss()\\n\",\n",
        "        \"\\n\",\n",
        "        \"for epoch in range(10):  # Reduced epochs\\n\",\n",
        "        \"    for batch in dataloader:\\n\",\n",
        "        \"        img, points, boxes = batch\\n\",\n",
        "        \"        img, points = img.cuda(), points.cuda()\\n\",\n",
        "        \"        yolo_out = yolo_model(img)\\n\",\n",
        "        \"        yolo_features = yolo_out[0].boxes.xywh\\n\",\n",
        "        \"        point_data = create_pointnet_data(points, boxes)\\n\",\n",
        "        \"        lidar_features = pointnet(point_data)\\n\",\n",
        "        \"        fused = fusion_model(yolo_features, lidar_features)\\n\",\n",
        "        \"        target = torch.zeros_like(fused)  # Dummy target\\n\",\n",
        "        \"        loss = criterion(fused, target)\\n\",\n",
        "        \"        optimizer.zero_grad()\\n\",\n",
        "        \"        loss.backward()\\n\",\n",
        "        \"        optimizer.step()\\n\",\n",
        "        \"    print(f'Epoch {epoch}, Loss: {loss.item()}')\\n\",\n",
        "        \"\\n\",\n",
        "        \"torch.save(fusion_model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt')\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwQTK7XcR1hQ",
        "outputId": "0275052c-3981-4783-da0a-cfc468a13374"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import torch\\n',\n",
              "    'import torch.nn as nn\\n',\n",
              "    'from ultralytics import YOLO\\n',\n",
              "    '\\n',\n",
              "    'class FusionTransformer(nn.Module):\\n',\n",
              "    '    def __init__(self, input_dim=68, hidden_dim=128, num_heads=4):  # Reduced size\\n',\n",
              "    '        super(FusionTransformer, self).__init__()\\n',\n",
              "    '        self.encoder = nn.TransformerEncoder(\\n',\n",
              "    '            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\\n',\n",
              "    '            num_layers=1  # Single layer for Colab\\n',\n",
              "    '        )\\n',\n",
              "    '        self.fc = nn.Linear(input_dim, 64)\\n',\n",
              "    '    \\n',\n",
              "    '    def forward(self, yolo_features, lidar_features):\\n',\n",
              "    '        combined = torch.cat((yolo_features, lidar_features), dim=-1)\\n',\n",
              "    '        fused = self.encoder(combined)\\n',\n",
              "    '        return self.fc(fused)\\n',\n",
              "    '\\n',\n",
              "    '# Load models\\n',\n",
              "    \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\",\n",
              "    'pointnet = PointNetSeg().cuda()\\n',\n",
              "    \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt'))\\n\",\n",
              "    'fusion_model = FusionTransformer().cuda()\\n',\n",
              "    '\\n',\n",
              "    'optimizer = torch.optim.Adam(fusion_model.parameters(), lr=0.001)\\n',\n",
              "    'criterion = nn.MSELoss()\\n',\n",
              "    '\\n',\n",
              "    'for epoch in range(10):  # Reduced epochs\\n',\n",
              "    '    for batch in dataloader:\\n',\n",
              "    '        img, points, boxes = batch\\n',\n",
              "    '        img, points = img.cuda(), points.cuda()\\n',\n",
              "    '        yolo_out = yolo_model(img)\\n',\n",
              "    '        yolo_features = yolo_out[0].boxes.xywh\\n',\n",
              "    '        point_data = create_pointnet_data(points, boxes)\\n',\n",
              "    '        lidar_features = pointnet(point_data)\\n',\n",
              "    '        fused = fusion_model(yolo_features, lidar_features)\\n',\n",
              "    '        target = torch.zeros_like(fused)  # Dummy target\\n',\n",
              "    '        loss = criterion(fused, target)\\n',\n",
              "    '        optimizer.zero_grad()\\n',\n",
              "    '        loss.backward()\\n',\n",
              "    '        optimizer.step()\\n',\n",
              "    \"    print(f'Epoch {epoch}, Loss: {loss.item()}')\\n\",\n",
              "    '\\n',\n",
              "    \"torch.save(fusion_model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt')\\n\"],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import gymnasium as gym\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"from stable_baselines3 import PPO\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"\\n\",\n",
        "        \"class ProxyDrivingEnv(gym.Env):\\n\",\n",
        "        \"    def __init__(self):\\n\",\n",
        "        \"        super(ProxyDrivingEnv, self).__init__()\\n\",\n",
        "        \"        self.env = gym.make('CarRacing-v2')\\n\",\n",
        "        \"        self.action_space = self.env.action_space\\n\",\n",
        "        \"        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(64,), dtype=np.float32)\\n\",\n",
        "        \"        self.fusion_model = FusionTransformer().cuda()\\n\",\n",
        "        \"        self.fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt'))\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def reset(self, seed=None):\\n\",\n",
        "        \"        obs, _ = self.env.reset(seed=seed)\\n\",\n",
        "        \"        return self._get_obs(obs), {}\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def step(self, action):\\n\",\n",
        "        \"        obs, reward, done, truncated, info = self.env.step(action)\\n\",\n",
        "        \"        return self._get_obs(obs), reward, done or truncated, info\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    def _get_obs(self, obs):\\n\",\n",
        "        \"        # Simulate fused features (replace with real YOLO/PointNet inputs if available)\\n\",\n",
        "        \"        dummy_yolo = torch.ones((1, 4)).cuda()\\n\",\n",
        "        \"        dummy_lidar = torch.ones((1, 64)).cuda()\\n\",\n",
        "        \"        fused = self.fusion_model(dummy_yolo, dummy_lidar)\\n\",\n",
        "        \"        return fused.detach().cpu().numpy().flatten()\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Train RL agent\\n\",\n",
        "        \"env = ProxyDrivingEnv()\\n\",\n",
        "        \"model = PPO('MlpPolicy', env, verbose=1)\\n\",\n",
        "        \"model.learn(total_timesteps=50000)  # Reduced timesteps\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Save model\\n\",\n",
        "        \"model.save('/content/drive/MyDrive/autonomous_ai/models/ppo_autonomous.zip')\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MinQAG7CR9fe",
        "outputId": "995d2c18-90ea-4418-c719-303fa2aef0c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import gymnasium as gym\\n',\n",
              "    'import numpy as np\\n',\n",
              "    'from stable_baselines3 import PPO\\n',\n",
              "    'import torch\\n',\n",
              "    '\\n',\n",
              "    'class ProxyDrivingEnv(gym.Env):\\n',\n",
              "    '    def __init__(self):\\n',\n",
              "    '        super(ProxyDrivingEnv, self).__init__()\\n',\n",
              "    \"        self.env = gym.make('CarRacing-v2')\\n\",\n",
              "    '        self.action_space = self.env.action_space\\n',\n",
              "    '        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(64,), dtype=np.float32)\\n',\n",
              "    '        self.fusion_model = FusionTransformer().cuda()\\n',\n",
              "    \"        self.fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt'))\\n\",\n",
              "    '    \\n',\n",
              "    '    def reset(self, seed=None):\\n',\n",
              "    '        obs, _ = self.env.reset(seed=seed)\\n',\n",
              "    '        return self._get_obs(obs), {}\\n',\n",
              "    '    \\n',\n",
              "    '    def step(self, action):\\n',\n",
              "    '        obs, reward, done, truncated, info = self.env.step(action)\\n',\n",
              "    '        return self._get_obs(obs), reward, done or truncated, info\\n',\n",
              "    '    \\n',\n",
              "    '    def _get_obs(self, obs):\\n',\n",
              "    '        # Simulate fused features (replace with real YOLO/PointNet inputs if available)\\n',\n",
              "    '        dummy_yolo = torch.ones((1, 4)).cuda()\\n',\n",
              "    '        dummy_lidar = torch.ones((1, 64)).cuda()\\n',\n",
              "    '        fused = self.fusion_model(dummy_yolo, dummy_lidar)\\n',\n",
              "    '        return fused.detach().cpu().numpy().flatten()\\n',\n",
              "    '\\n',\n",
              "    '# Train RL agent\\n',\n",
              "    'env = ProxyDrivingEnv()\\n',\n",
              "    \"model = PPO('MlpPolicy', env, verbose=1)\\n\",\n",
              "    'model.learn(total_timesteps=50000)  # Reduced timesteps\\n',\n",
              "    '\\n',\n",
              "    '# Save model\\n',\n",
              "    \"model.save('/content/drive/MyDrive/autonomous_ai/models/ppo_autonomous.zip')\\n\"],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import torch\\n\",\n",
        "        \"from ultralytics import YOLO\\n\",\n",
        "        \"import time\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Load models\\n\",\n",
        "        \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\",\n",
        "        \"pointnet = PointNetSeg().cuda()\\n\",\n",
        "        \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt'))\\n\",\n",
        "        \"fusion_model = FusionTransformer().cuda()\\n\",\n",
        "        \"fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt'))\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Test inference latency\\n\",\n",
        "        \"x_img = torch.ones((1, 3, 640, 640)).cuda()\\n\",\n",
        "        \"x_points = torch.ones((1, 1000, 3)).cuda()\\n\",\n",
        "        \"x_fusion = torch.ones((1, 68)).cuda()\\n\",\n",
        "        \"\\n\",\n",
        "        \"start = time.time()\\n\",\n",
        "        \"yolo_model(x_img)\\n\",\n",
        "        \"pointnet(create_pointnet_data(x_points, None))\\n\",\n",
        "        \"fusion_model(x_fusion, x_fusion)\\n\",\n",
        "        \"latency = (time.time() - start) * 1000\\n\",\n",
        "        \"print(f'Inference time: {latency:.2f} ms')\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Save models (already optimized for size)\\n\",\n",
        "        \"torch.save(yolo_model.model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/yolo_optimized.pt')\\n\",\n",
        "        \"torch.save(pointnet.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/pointnet_optimized.pt')\\n\",\n",
        "        \"torch.save(fusion_model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/fusion_optimized.pt')\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erceDB_ySEdO",
        "outputId": "b8556b5f-bdc6-4735-8da4-373c0d627dd1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import torch\\n',\n",
              "    'from ultralytics import YOLO\\n',\n",
              "    'import time\\n',\n",
              "    '\\n',\n",
              "    '# Load models\\n',\n",
              "    \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_autonomous.pt')\\n\",\n",
              "    'pointnet = PointNetSeg().cuda()\\n',\n",
              "    \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_seg.pt'))\\n\",\n",
              "    'fusion_model = FusionTransformer().cuda()\\n',\n",
              "    \"fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_transformer.pt'))\\n\",\n",
              "    '\\n',\n",
              "    '# Test inference latency\\n',\n",
              "    'x_img = torch.ones((1, 3, 640, 640)).cuda()\\n',\n",
              "    'x_points = torch.ones((1, 1000, 3)).cuda()\\n',\n",
              "    'x_fusion = torch.ones((1, 68)).cuda()\\n',\n",
              "    '\\n',\n",
              "    'start = time.time()\\n',\n",
              "    'yolo_model(x_img)\\n',\n",
              "    'pointnet(create_pointnet_data(x_points, None))\\n',\n",
              "    'fusion_model(x_fusion, x_fusion)\\n',\n",
              "    'latency = (time.time() - start) * 1000\\n',\n",
              "    \"print(f'Inference time: {latency:.2f} ms')\\n\",\n",
              "    '\\n',\n",
              "    '# Save models (already optimized for size)\\n',\n",
              "    \"torch.save(yolo_model.model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/yolo_optimized.pt')\\n\",\n",
              "    \"torch.save(pointnet.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/pointnet_optimized.pt')\\n\",\n",
              "    \"torch.save(fusion_model.state_dict(), '/content/drive/MyDrive/autonomous_ai/models/fusion_optimized.pt')\\n\"],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import gymnasium as gym\\n\",\n",
        "        \"from ultralytics import YOLO\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"from stable_baselines3 import PPO\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Load models\\n\",\n",
        "        \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_optimized.pt')\\n\",\n",
        "        \"pointnet = PointNetSeg().cuda()\\n\",\n",
        "        \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_optimized.pt'))\\n\",\n",
        "        \"fusion_model = FusionTransformer().cuda()\\n\",\n",
        "        \"fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_optimized.pt'))\\n\",\n",
        "        \"rl_model = PPO.load('/content/drive/MyDrive/autonomous_ai/models/ppo_autonomous.zip')\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Test in CarRacing-v2\\n\",\n",
        "        \"env = gym.make('CarRacing-v2', render_mode='human')\\n\",\n",
        "        \"obs, _ = env.reset()\\n\",\n",
        "        \"for _ in range(1000):\\n\",\n",
        "        \"    img = torch.tensor(obs.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).cuda()\\n\",\n",
        "        \"    yolo_out = yolo_model(img)\\n\",\n",
        "        \"    dummy_points = torch.ones((1, 1000, 3)).cuda()\\n\",\n",
        "        \"    point_data = create_pointnet_data(dummy_points, None)\\n\",\n",
        "        \"    lidar_out = pointnet(point_data)\\n\",\n",
        "        \"    fused = fusion_model(yolo_out[0].boxes.xywh, lidar_out)\\n\",\n",
        "        \"    action, _ = rl_model.predict(fused.detach().cpu().numpy())\\n\",\n",
        "        \"    obs, reward, done, truncated, _ = env.step(action)\\n\",\n",
        "        \"    if done or truncated:\\n\",\n",
        "        \"        obs, _ = env.reset()\\n\",\n",
        "        \"env.close()\\n\"\n",
        "      ],\n",
        "      \"metadata\": {},\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfuquVOpSKnY",
        "outputId": "63d04ffa-caa9-4f85-f57c-981bd7872828"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'source': ['import gymnasium as gym\\n',\n",
              "    'from ultralytics import YOLO\\n',\n",
              "    'import torch\\n',\n",
              "    'from stable_baselines3 import PPO\\n',\n",
              "    '\\n',\n",
              "    '# Load models\\n',\n",
              "    \"yolo_model = YOLO('/content/drive/MyDrive/autonomous_ai/models/yolo_optimized.pt')\\n\",\n",
              "    'pointnet = PointNetSeg().cuda()\\n',\n",
              "    \"pointnet.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/pointnet_optimized.pt'))\\n\",\n",
              "    'fusion_model = FusionTransformer().cuda()\\n',\n",
              "    \"fusion_model.load_state_dict(torch.load('/content/drive/MyDrive/autonomous_ai/models/fusion_optimized.pt'))\\n\",\n",
              "    \"rl_model = PPO.load('/content/drive/MyDrive/autonomous_ai/models/ppo_autonomous.zip')\\n\",\n",
              "    '\\n',\n",
              "    '# Test in CarRacing-v2\\n',\n",
              "    \"env = gym.make('CarRacing-v2', render_mode='human')\\n\",\n",
              "    'obs, _ = env.reset()\\n',\n",
              "    'for _ in range(1000):\\n',\n",
              "    '    img = torch.tensor(obs.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).cuda()\\n',\n",
              "    '    yolo_out = yolo_model(img)\\n',\n",
              "    '    dummy_points = torch.ones((1, 1000, 3)).cuda()\\n',\n",
              "    '    point_data = create_pointnet_data(dummy_points, None)\\n',\n",
              "    '    lidar_out = pointnet(point_data)\\n',\n",
              "    '    fused = fusion_model(yolo_out[0].boxes.xywh, lidar_out)\\n',\n",
              "    '    action, _ = rl_model.predict(fused.detach().cpu().numpy())\\n',\n",
              "    '    obs, reward, done, truncated, _ = env.step(action)\\n',\n",
              "    '    if done or truncated:\\n',\n",
              "    '        obs, _ = env.reset()\\n',\n",
              "    'env.close()\\n'],\n",
              "   'metadata': {},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}